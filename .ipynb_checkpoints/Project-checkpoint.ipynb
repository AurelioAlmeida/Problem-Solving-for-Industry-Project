{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3d7692d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49152aa0",
   "metadata": {},
   "source": [
    "### Define the path to collect and save PDF files from One drive to Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8f433b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the OneDrive directory paths\n",
    "onedrive_path_1 = \"/Users/rennersantana/OneDrive - TestReach/Briefs\"\n",
    "onedrive_path_2 = \"/Users/rennersantana/OneDrive - TestReach/Briefs/On-demand exam briefs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1958e",
   "metadata": {},
   "source": [
    "### Path to save collected PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39ddf3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the collected PDF files\n",
    "save_path = \"/Users/rennersantana/Desktop/AI - ChatBot/PDF_Files\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378eb58",
   "metadata": {},
   "source": [
    "### Collect PDF files recursively from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c886c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect PDF files recursively from a directory\n",
    "def collect_pdf_files(directory):\n",
    "    pdf_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                pdf_files.append(os.path.join(root, file))\n",
    "    return pdf_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3834c2fa",
   "metadata": {},
   "source": [
    "### Copy PDf files to a specified directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "48d45dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy PDF files to a specified directory\n",
    "def copy_pdf_files(pdf_files, save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    for pdf_file in pdf_files:\n",
    "        shutil.copy(pdf_file, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c433ef",
   "metadata": {},
   "source": [
    "### Here we collect the PDf files from both directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "891b27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we collect the PDF files from both directories\n",
    "pdf_files_1 = collect_pdf_files(onedrive_path_1)\n",
    "pdf_files_2 = collect_pdf_files(onedrive_path_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bea14d",
   "metadata": {},
   "source": [
    "### Combine the lists of PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7bb950f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the lists of PDF files\n",
    "pdf_files = pdf_files_1 + pdf_files_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd314fe",
   "metadata": {},
   "source": [
    "### Copy PDF files to a specified directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e16a7390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy PDF files to the specified directory\n",
    "copy_pdf_files(pdf_files, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd2e781",
   "metadata": {},
   "source": [
    "### Print the list of collected PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c8f6ffdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF files copied to: /Users/rennersantana/Desktop/AI - ChatBot/PDF_Files\n"
     ]
    }
   ],
   "source": [
    "# Print the list of collected PDF files\n",
    "print(\"PDF files copied to:\", save_path)\n",
    "#for pdf_file in pdf_files:\n",
    "    #print(pdf_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c9ac67",
   "metadata": {},
   "source": [
    "## Start of the code after copy PDF file localy to a specified directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ab240f",
   "metadata": {},
   "source": [
    "### Set the directory containing PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b9bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing PDF files\n",
    "pdf_directory = \"/Users/rennersantana/OneDrive - TestReach/Briefs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bbcbd1",
   "metadata": {},
   "source": [
    "### Check if the directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f484b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the PDF directory exists; if not, print a message.\n",
    "if not os.path.exists(pdf_directory):\n",
    "    print(f\"Directory '{pdf_directory}' does not exist.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604df987",
   "metadata": {},
   "source": [
    "### Definingh function to collect Specific Information from PDf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b68ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to collect specific information from PDF files\n",
    "def extract_information_from_pdf(pdf_file):\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        # Initialize variables to store extracted information\n",
    "        on_screen_calculator = \"Not Available\"\n",
    "        on_screen_notepad = \"Not Available\"\n",
    "        comfort_breaks = \"Not Allowed\"\n",
    "        identification = {\n",
    "            \"Passport\": \"Not Available\",\n",
    "            \"Driver’s License\": \"Not Available\",\n",
    "            \"National ID card\": \"Not Available\",\n",
    "            \"EU ID card\": \"Not Available\",\n",
    "            \"Work ID\": \"Not Available\"\n",
    "        }\n",
    "        # Loop through each page of the PDF to extract information\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            # Extract on-screen calculator information\n",
    "            if \"On Screen Calculator: Available\" in text:\n",
    "                on_screen_calculator = \"Available\"\n",
    "            # Extract on-screen notepad information\n",
    "            if \"On Screen Notepad: Available\" in text:\n",
    "                on_screen_notepad = \"Available\"\n",
    "            # Extract comfort breaks information\n",
    "            if \"Comfort Breaks\" in text:\n",
    "                if any(phrase in text for phrase in [\"Allowed\", \"Y\", \"YES\", \"yes\"]):\n",
    "                    comfort_breaks = \"Allowed\"\n",
    "                else:\n",
    "                    comfort_breaks = \"Not Allowed\"\n",
    "            # Extract identification information\n",
    "            for item in identification:\n",
    "                if item in text:\n",
    "                    start_idx = text.find(item)\n",
    "                    end_idx = text.find(\"\\n\", start_idx)\n",
    "                    line = text[start_idx:end_idx]\n",
    "                    if any(phrase in line for phrase in [\"Y\", \"YES\", \"yes\", \"Allowed\", \"Not Allowed\", \"allowed\", \"not allowed\"]):\n",
    "                        identification[item] = \"Yes\" if any(phrase in line for phrase in [\"Y\", \"YES\", \"yes\", \"Allowed\", \"allowed\"]) else \"No\"\n",
    "        # Return the extracted information\n",
    "        return on_screen_calculator, on_screen_notepad, comfort_breaks, identification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e23cea",
   "metadata": {},
   "source": [
    "### Initialise lists to store extracted information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3443b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store extracted information\n",
    "filename_list = []\n",
    "calculator_list = []\n",
    "notepad_list = []\n",
    "comfort_breaks_list = []\n",
    "passport_list = []\n",
    "drivers_license_list = []\n",
    "national_id_card_list = []\n",
    "eu_id_card_list = []\n",
    "work_id_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379bf59b",
   "metadata": {},
   "source": [
    "### Loop through each PDf file in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccd9c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each PDF file in the directory\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_file = os.path.join(pdf_directory, filename)\n",
    "        # Extract information from the current PDF file\n",
    "        on_screen_calculator, on_screen_notepad, comfort_breaks, identification = extract_information_from_pdf(pdf_file)\n",
    "        # Append extracted information to lists\n",
    "        filename_list.append(filename)\n",
    "        calculator_list.append(on_screen_calculator)\n",
    "        notepad_list.append(on_screen_notepad)\n",
    "        comfort_breaks_list.append(comfort_breaks)\n",
    "        passport_list.append(identification[\"Passport\"])\n",
    "        drivers_license_list.append(identification[\"Driver’s License\"])\n",
    "        national_id_card_list.append(identification[\"National ID card\"])\n",
    "        eu_id_card_list.append(identification[\"EU ID card\"])\n",
    "        work_id_list.append(identification[\"Work ID\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b28b393",
   "metadata": {},
   "source": [
    "### Create a DataFrame to store extracted information in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bee6fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the extracted information\n",
    "data = {\n",
    "    \"Filename\": filename_list,\n",
    "    \"On Screen Calculator\": calculator_list,\n",
    "    \"On Screen Notepad\": notepad_list,\n",
    "    \"Comfort Breaks\": comfort_breaks_list,\n",
    "    \"Passport\": passport_list,\n",
    "    \"Driver’s License\": drivers_license_list,\n",
    "    \"National ID card\": national_id_card_list,\n",
    "    \"EU ID card\": eu_id_card_list,\n",
    "    \"Work ID\": work_id_list\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc40d1d",
   "metadata": {},
   "source": [
    "### Path to save the CSV file once extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21a118c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to save the CSV file\n",
    "csv_file_path = \"/Users/rennersantana/Desktop/AI - ChatBot/extracted_information.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70a8ff",
   "metadata": {},
   "source": [
    "### Save DataFrame to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a4dd2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a3b3d4",
   "metadata": {},
   "source": [
    "### Print confirmation file was saved in the specific path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65863543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted information saved to: /Users/rennersantana/Desktop/AI - ChatBot/extracted_information.csv\n"
     ]
    }
   ],
   "source": [
    "# Print confirmation message\n",
    "print(\"Extracted information saved to:\", csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda31c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e72469f7",
   "metadata": {},
   "source": [
    "### Preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1fd24a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. Here's a preview:\n",
      "                                            Filename On Screen Calculator  \\\n",
      "0                             EMA 23 April, 2024.pdf            Available   \n",
      "1  SRB - Exam Brief_Senior Bank Resolution Expert...        Not Available   \n",
      "2                      FSEM Exam Brief 2024.docx.pdf        Not Available   \n",
      "3                                CIPS Brief 2024.pdf            Available   \n",
      "4                       ABP Brief 26 April, 2024.pdf        Not Available   \n",
      "\n",
      "  On Screen Notepad Comfort Breaks Passport Driver’s License National ID card  \\\n",
      "0         Available    Not Allowed      Yes              Yes              Yes   \n",
      "1         Available    Not Allowed      Yes              Yes              Yes   \n",
      "2     Not Available    Not Allowed      Yes              Yes              Yes   \n",
      "3         Available    Not Allowed      Yes              Yes              Yes   \n",
      "4     Not Available    Not Allowed      Yes    Not Available              Yes   \n",
      "\n",
      "  EU ID card        Work ID  \n",
      "0        Yes  Not Available  \n",
      "1        Yes  Not Available  \n",
      "2        Yes  Not Available  \n",
      "3        Yes  Not Available  \n",
      "4        Yes  Not Available  \n",
      "\n",
      "Text data standardized. Here's a preview:\n",
      "                                            Filename On Screen Calculator  \\\n",
      "0                             EMA 23 April, 2024.pdf            Available   \n",
      "1  SRB - Exam Brief_Senior Bank Resolution Expert...        Not Available   \n",
      "2                      FSEM Exam Brief 2024.docx.pdf        Not Available   \n",
      "3                                CIPS Brief 2024.pdf            Available   \n",
      "4                       ABP Brief 26 April, 2024.pdf        Not Available   \n",
      "\n",
      "  On Screen Notepad Comfort Breaks Passport Driver’s License National ID card  \\\n",
      "0         Available    Not Allowed      Yes              Yes              Yes   \n",
      "1         Available    Not Allowed      Yes              Yes              Yes   \n",
      "2     Not Available    Not Allowed      Yes              Yes              Yes   \n",
      "3         Available    Not Allowed      Yes              Yes              Yes   \n",
      "4     Not Available    Not Allowed      Yes    Not Available              Yes   \n",
      "\n",
      "  EU ID card        Work ID  \n",
      "0        Yes  Not Available  \n",
      "1        Yes  Not Available  \n",
      "2        Yes  Not Available  \n",
      "3        Yes  Not Available  \n",
      "4        Yes  Not Available  \n",
      "\n",
      "Categorical data encoded. Here's a preview of the new columns:\n",
      "Index(['Filename', 'On Screen Calculator_Available',\n",
      "       'On Screen Calculator_Not Available', 'On Screen Notepad_Available',\n",
      "       'On Screen Notepad_Not Available', 'Comfort Breaks_Allowed',\n",
      "       'Comfort Breaks_Not Allowed', 'Passport_Not Available', 'Passport_Yes',\n",
      "       'Driver’s License_Not Available', 'Driver’s License_Yes',\n",
      "       'National ID card_Not Available', 'National ID card_Yes',\n",
      "       'EU ID card_Not Available', 'EU ID card_Yes', 'Work ID_Not Available',\n",
      "       'Work ID_Yes'],\n",
      "      dtype='object')\n",
      "\n",
      "Preprocessed data saved to '/Users/rennersantana/Desktop/AI - ChatBot/preprocessed_extracted_information.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"/Users/rennersantana/Desktop/AI - ChatBot/extracted_information.csv\")\n",
    "print(\"Data loaded successfully. Here's a preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Standardizing text data\n",
    "df['On Screen Calculator'] = df['On Screen Calculator'].str.title()  # Ensures consistent casing\n",
    "df['On Screen Notepad'] = df['On Screen Notepad'].str.title()\n",
    "df['Comfort Breaks'] = df['Comfort Breaks'].str.title()\n",
    "print(\"\\nText data standardized. Here's a preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Encoding categorical data using one-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=[\n",
    "    'On Screen Calculator', 'On Screen Notepad', 'Comfort Breaks',\n",
    "    'Passport', 'Driver’s License', 'National ID card', 'EU ID card', 'Work ID'\n",
    "])\n",
    "print(\"\\nCategorical data encoded. Here's a preview of the new columns:\")\n",
    "print(df_encoded.columns)\n",
    "\n",
    "# Save the preprocessed data\n",
    "df_encoded.to_csv(\"/Users/rennersantana/Desktop/AI - ChatBot/preprocessed_extracted_information.csv\", index=False)\n",
    "print(\"\\nPreprocessed data saved to '/Users/rennersantana/Desktop/AI - ChatBot/preprocessed_extracted_information.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a23111",
   "metadata": {},
   "source": [
    "### Encoding and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "353bec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the preprocessed data\n",
    "df = pd.read_csv(\"/Users/rennersantana/Desktop/AI - ChatBot/preprocessed_extracted_information.csv\")\n",
    "\n",
    "# Define the target variable (make sure to replace 'Comfort_Breaks_Allowed' with the actual column name)\n",
    "target = 'Comfort Breaks_Allowed'  # This should be replaced with the actual target column name\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(target, axis=1)  # This drops your target column from the feature set\n",
    "y = df[target]  # This is your target variable\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, you can proceed with model training using X_train, y_train and then evaluate with X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f3b1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "data_path = \"/Users/rennersantana/Desktop/AI - ChatBot/preprocessed_extracted_information.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Example: Let's assume you want to predict 'Comfort Breaks_Allowed' from the other columns\n",
    "# First, prepare your feature columns and the label column\n",
    "feature_columns = [col for col in df.columns if col != 'Comfort Breaks_Allowed']\n",
    "X = df[feature_columns]\n",
    "y = df['Comfort Breaks_Allowed']\n",
    "\n",
    "# Tokenize the feature data (Note: BERT tokenizer is typically used for text data. Here, we're adapting it just for demonstration; this is non-standard.)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Simulate tokenization (adapt as needed, this is just a placeholder)\n",
    "encoded_features = [{\"input_ids\": torch.tensor([1]), \"attention_mask\": torch.tensor([1])} for _ in range(len(X))]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_features, temp_features, train_labels, temp_labels = train_test_split(encoded_features, y, test_size=0.3, random_state=42)\n",
    "val_features, test_features, val_labels, test_labels = train_test_split(temp_features, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Continue with setting up the datasets for training\n",
    "class ExamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Convert encoded features into datasets\n",
    "train_dataset = ExamDataset(train_features, train_labels)\n",
    "val_dataset = ExamDataset(val_features, val_labels)\n",
    "test_dataset = ExamDataset(test_features, test_labels)\n",
    "\n",
    "# You would then proceed to define the model, training arguments, and train the model as previously detailed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01cbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
